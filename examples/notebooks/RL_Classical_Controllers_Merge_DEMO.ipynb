{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2888dffe",
   "metadata": {},
   "source": [
    "## Train an RL agent interacting with a stable grid\n",
    "In this notebook will be shown how to control one source with an RL agent learning a task connected to a stable grid provided by  a classic controller. The topics covered are:\n",
    "\n",
    " - ### Experiment merging classic controllers and RL agents\n",
    " - ### Reward and featurize functions with named policys\n",
    " - ### MultiController\n",
    " - ### Training an RL agent in a classicly controlled grid\n",
    "\n",
    "## Experiment merging classic controllers and RL agents\n",
    "\n",
    "In this notebook a reinforcement learning agent is trained to draw current from a stable grid.\n",
    "The 3-phase electric power grid is formed by a classic controller in open-loop (`Swing`) mode. \n",
    "For more details about how the classic control works, see `Classical_Controllers_Introduction.ipynb`.\n",
    "\n",
    "The use case is shown in the figure below.\n",
    "This environment consists of a 3-phase electrical power grid with 2 sources connected via a cable.\n",
    "\n",
    "![](figures/RL_classic_swing.png \"\")\n",
    "\n",
    "The first source is controlled by the RL agent `my_ddpg` which should learn to draw power from the grid, therefore act like an active load.\n",
    "The secound source is controlled by a classic controller in open-loop. \n",
    "The Swing mode is used to create a stable 3-phase grid to supply the load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e01153",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f3322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  \"source\" => Any[Dict{Any, Any}(\"control_type\"=>\"RL\", \"mode\"=>\"my_ddpg\", \"fltr…\n",
       "  \"grid\"   => Dict{Any, Any}(\"phase\"=>3, \"ramp_end\"=>0.04)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = \n",
    "Dict{Any, Any}(\n",
    "    \"source\" => Any[\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3, \n",
    "                        \"control_type\" => \"RL\", \n",
    "                        \"mode\" => \"my_ddpg\", \n",
    "                        \"fltr\" => \"L\"),\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3, \n",
    "                        \"fltr\" => \"LC\", \n",
    "                        \"control_type\" => \n",
    "                        \"classic\", \"mode\" => 1),\n",
    "                    ],\n",
    "    \"grid\" => Dict{Any, Any}(\n",
    "        \"phase\" => 3, \n",
    "        \"ramp_end\" => 0.04,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039119e",
   "metadata": {},
   "source": [
    "Like already introduced in `RL_Single_Agent_DEMO.ipynb`, an appropriate `reference(t)` function has to be defined to communicate the wanted task to learn to the agent.\n",
    "In this example we will use the time `t` handed over to the function to generate time-variing reference signals.\n",
    "In more detail a three-phase sinusoidal refrence signal shifted by 120° and an aplitude of 10 A is created.\n",
    "This should teach the agent to draw time-variing current from the grid. \n",
    "The phase is thereby chosen similar to the definition in the `Swing` mode.\n",
    "\n",
    "$i_\\mathrm{L,ref} =  - 10 \\,\\text{cos}\\left(2 \\pi \\,50 \\, t - \\frac{2}{3} \\pi (n-1) \\right)$, with $n \\in [0,1,2]$.\n",
    "\n",
    "Here, n represents the index refering to the 3 phases of the grid.\n",
    "\n",
    "For more enhanced reference functions, the reference current could be chosen with regards to power (active and reactive) reference values.\n",
    "Feel free to implement, change and contribute! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf3c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reference(t)\n",
    "    θ = 2*pi*50*t\n",
    "    θph = [θ; θ - 120π/180; θ + 120π/180]\n",
    "    return -10 * cos.(θph) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9ce89",
   "metadata": {},
   "source": [
    "## Reward and featurize functions with named policys\n",
    "\n",
    "Afterwards the `featurize()` function adds the signal generated by the `reference` function to the state for the agent `my_ddpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23420f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#9 (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_ddpg\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45180b",
   "metadata": {},
   "source": [
    "Then the `reward()` function is defined. Here, again it is based on the root-mean square error (RMSE) teach the agent `my_ddpg` to match the reference signal to the measured signal. \n",
    "\n",
    "If the measured state is greater than `1`. In that case a punishment is returned which, here, is chosen to be `r = -1`.\n",
    "It not and if the measured value differs from the reference, the average error is substracted from the maximal reward: `r = 1 - RMSE`:\n",
    "\n",
    "$r = 1 - \\frac{1}{3} \\sum_{{p \\in \\{\\mathrm{a,b,c}\\}}} \\sqrt{\\frac{|i_\\mathrm{L,ref,p} - i_\\mathrm{L1,p}|}{2}}$\n",
    "\n",
    "This function is only used if the name of the policy is `my_ddpg` which was chosen in the parameter dict. \n",
    "In any other case, 1 is returned.\n",
    "This could be used to define 2 different reward functions for 2 different agents via name (e.g., `my_ddpg` and `my_sac`) to learn for example a current control with the `my_ddpg` agent but a voltage control task with the `my_sac` agent.\n",
    "\n",
    "Here, in any case but `name == my_ddpg` - so in case of the `classic` controller `r = 1` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5137a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward_function (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward_function(env, name = nothing)\n",
    "    if name == \"my_ddpg\"\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1_a\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source1_i_L1_b\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source1_i_L1_c\", env.state_ids)]\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "            return r \n",
    "        end\n",
    "    else\n",
    "        return 1\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb0752",
   "metadata": {},
   "source": [
    "Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electircal power grid. Here, no CM Matrix defining the connection is used. Since the grid consists only of 2 sources there is only connection possible. The `ElectricGridEnv` creates this internally based on the length of the parameter dict sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d1d66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "KeyError: key 1 not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key 1 not found",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ C:\\Users\\marmey\\.julia\\packages\\JuMP\\9CBpS\\src\\Containers\\DenseAxisArray.jl:78 [inlined]",
      " [2] _getindex_recurse",
      "   @ C:\\Users\\marmey\\.julia\\packages\\JuMP\\9CBpS\\src\\Containers\\DenseAxisArray.jl:330 [inlined]",
      " [3] to_index",
      "   @ C:\\Users\\marmey\\.julia\\packages\\JuMP\\9CBpS\\src\\Containers\\DenseAxisArray.jl:339 [inlined]",
      " [4] getindex",
      "   @ C:\\Users\\marmey\\.julia\\packages\\JuMP\\9CBpS\\src\\Containers\\DenseAxisArray.jl:346 [inlined]",
      " [5] LayoutCabels(CM::Matrix{Float64}, num_source::Int64, num_load::Int64, parameters::Dict{Any, Any}, verbosity::Int64)",
      "   @ JEG c:\\Users\\marmey\\Documents\\dare\\src\\power_system_theory.jl:942",
      " [6] CheckParameters(parameters::Dict{Any, Any}, num_sources::Int64, num_loads::Int64, num_connections::Int64, CM::Matrix{Float64}, ts::Float64, verbosity::Int64)",
      "   @ JEG c:\\Users\\marmey\\Documents\\dare\\src\\node_constructor.jl:808",
      " [7] NodeConstructor(; num_sources::Int64, num_loads::Int64, CM::Nothing, parameters::Dict{Any, Any}, S2S_p::Int64, S2L_p::Int64, L2L_p::Int64, ts::Float64, verbosity::Int64)",
      "   @ JEG c:\\Users\\marmey\\Documents\\dare\\src\\node_constructor.jl:70",
      " [8] ElectricGridEnv(; maxsteps::Int64, ts::Float64, action_space::Nothing, state_space::Nothing, prepare_action::Nothing, featurize::var\"#9#10\", reward_function::Function, CM::Nothing, num_sources::Nothing, num_loads::Nothing, parameters::Dict{Any, Any}, x0::Nothing, t0::Float64, state_ids::Nothing, convert_state_to_cpu::Bool, use_gpu::Bool, reward::Nothing, action::Nothing, action_ids::Nothing, action_delay::Int64, t_end::Float64, verbosity::Int64, agent_dict::Nothing)",
      "   @ JEG c:\\Users\\marmey\\Documents\\dare\\src\\electric_grid_env.jl:151",
      " [9] top-level scope",
      "   @ In[17]:1"
     ]
    }
   ],
   "source": [
    "env = ElectricGridEnv(\n",
    "    parameters = parameters, \n",
    "    t_end = 0.1, \n",
    "    featurize = featurize_ddpg, \n",
    "    reward_function = reward_function, \n",
    "    action_delay = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ded2d",
   "metadata": {},
   "source": [
    "## MultiController\n",
    "\n",
    "Like already shon in `RL_Single_Agent_DEMO.ipynb`, again an DDPG agent is created. \n",
    "The `SetupAgents()` function is then used to configure the `controllers` utilizing the `MultiController`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1089d694",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: env not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: env not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[18]:1"
     ]
    }
   ],
   "source": [
    "agent = CreateAgentDdpg(na = length(env.agent_dict[\"my_ddpg\"][\"action_ids\"]),\n",
    "                          ns = length(state(env, \"my_ddpg\")),\n",
    "                          use_gpu = false)\n",
    "\n",
    "my_custom_agents = Dict(\"my_ddpg\" => agent)\n",
    "\n",
    "controllers = SetupAgents(env, my_custom_agents);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bf166",
   "metadata": {},
   "source": [
    "Like shown in the following figure, the `controllers` struct constits now of 2 agents - one per source.\n",
    "\n",
    "![](figures/Multiagent_classic_RL.png \"\")\n",
    "\n",
    "Since we have defined 2 sources in the env, one controller classically and the other by RL, the `MultiController` hands over the correct indices of the environment to the controllers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7b14b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: controllers not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: controllers not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[19]:1"
     ]
    }
   ],
   "source": [
    "controllers.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a41629",
   "metadata": {},
   "source": [
    "This enables each controller, e.g., to find the correct states in the environment state.\n",
    "In the parameter dict the first source is labeled to be controlled by the RL agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76112dc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: controllers not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: controllers not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:1"
     ]
    }
   ],
   "source": [
    "controllers.agents[\"my_ddpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371d587",
   "metadata": {},
   "source": [
    "The secound source is controlled via the classic controller: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6efb40db",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: controllers not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: controllers not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[21]:1"
     ]
    }
   ],
   "source": [
    "controllers.agents[\"classic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab250b",
   "metadata": {},
   "source": [
    "## Training an RL agent in a classicly controlled grid\n",
    "\n",
    "Then the `Learn()` function is used to train 20 episodes. \n",
    "Here only the RL agent is trained. \n",
    "The classic controller is executed to control the secound source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905ff2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: controllers not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: controllers not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[22]:1"
     ]
    }
   ],
   "source": [
    "Learn(controllers, env, num_episodes = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56598c5",
   "metadata": {},
   "source": [
    "In the output the two different rewards for `classic` and `my_ddpg` can be identified.\n",
    "Shown is the accumulated reward per episode. Since the `classic` gets `r = 1` per step, this reward correlates with the length of the episode while the reward for the `my_ddpg` agent is depending on the above described reward function.\n",
    "After the training, the `Simulate()` function is used to run a test epiode without action noise and the state to be controlled ($i_\\mathrm{L1}$) is plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d35577b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: env not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: env not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[24]:1"
     ]
    }
   ],
   "source": [
    "hook = DataHook(collect_state_ids = env.state_ids,\n",
    "                collect_action_ids = env.action_ids)\n",
    "\n",
    "hook = Simulate(controllers, env, hook=hook)\n",
    "\n",
    "\n",
    "RenderHookResults(hook = hook,\n",
    "                    states_to_plot  = env.state_ids,\n",
    "                    actions_to_plot = env.action_ids,\n",
    "                    plot_reward=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b11b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
