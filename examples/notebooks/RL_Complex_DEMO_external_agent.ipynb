{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we want to set up a JuliaElectricGrid simulation utilizing a reinforcement learning agent from `ReinforcementLearning.jl`.\n",
    "We start with the same code we used in the `Reinforcement Learning in Larger Grids` section (or in the `RL_Complex_DEMO` notebook, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "CABLE LAYOUT BASED ON POWER FLOW EQUATIONS:\n",
      "termination_status = INVALID_MODEL\n",
      "primal_status      = UNKNOWN_RESULT_STATUS\n",
      "objective_value    = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer_status[\"termination_status\"] = MathOptInterface.INVALID_MODEL\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-dimensional DenseAxisArray{Float64,2,...} with index sets:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dimension 1, Base.OneTo(3)\n",
      "    Dimension 2, [\"v\", \"theta\", \"P\", \"Q\"]\n",
      "And data, a 3×4 Matrix{Float64}:\n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.0  0.0  0.0  0.0\n",
      " 230.0  0.0  0.0  0.0\n",
      " 230.0  0.0  0.0  0.0\n",
      "asdasdasd\n",
      "\n",
      "2-dimensional DenseAxisArray{Float64,2,...} with index sets:\n",
      "    Dimension 1, Base.OneTo(2)\n",
      "    Dimension 2, [\"radius\", \"D-to-neutral\"]\n",
      "And data, a 2×2 Matrix{Float64}:\n",
      " 0.0375  0.1922763744\n",
      " 0.0375  0.1922763744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Power flow equation not solveable! Maybe parameter setting invalid.\n",
      "│                   Default values are used for cable parameters\n",
      "└ @ JEG e:\\Documents\\dare\\src\\power_system_theory.jl:1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Normalization is done based on the defined parameter limits.\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:329\n",
      "┌ Info: Time simulation run time: 1.0 [s] ~> 10001 steps\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:330\n"
     ]
    }
   ],
   "source": [
    "using JEG\n",
    "using ReinforcementLearning\n",
    "using StableRNGs\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using IntervalSets\n",
    "\n",
    "CM = [0.0   1.0  0\n",
    "     -1.0   0.0  2.0\n",
    "     0  -2.0  0.0]\n",
    "\n",
    "parameters =\n",
    "Dict{Any, Any}(\n",
    "    \"source\" => Any[\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"control_type\" => \"RL\",\n",
    "                        \"mode\" => \"my_ddpg\",\n",
    "                        \"fltr\" => \"L\",\n",
    "                        \"i_limit\"      => 2000.),\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"fltr\" => \"LC\",\n",
    "                        \"control_type\" =>\n",
    "                        \"RL\", \"mode\" => \"my_ddpg\"),\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"fltr\" => \"L\",\n",
    "                        \"control_type\" =>\n",
    "                        \"RL\", \"mode\" => \"my_ddpg\",\n",
    "                        \"i_limit\"      => 2000.),\n",
    "                    ],\n",
    "        #\"load\"   => Any[\n",
    "        #    Dict{Any, Any}(\"impedance\" => \"RLC\", \"R\" => R_load, \"v_limit\" => 1e4, \"i_limit\" => 1e4)\n",
    "        #    ],\n",
    "    \"grid\" => Dict{Any, Any}(\n",
    "        \"phase\" => 1,\n",
    "        \"ramp_end\" => 0.04,)\n",
    ")\n",
    "\n",
    "function reference(t)\n",
    "\n",
    "    #return [-1., -2.]\n",
    "    return [-10., 230., -15.]\n",
    "end\n",
    "\n",
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_ddpg\"\n",
    "        #norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        #state = vcat(state, reference(env.t)/norm_ref)\n",
    "\n",
    "        refs = reference(env.t)\n",
    "        refs[1] = refs[1] / env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        refs[2] = refs[2] / env.nc.parameters[\"source\"][2][\"v_limit\"]\n",
    "        refs[3] = refs[3] / env.nc.parameters[\"source\"][3][\"i_limit\"]\n",
    "\n",
    "        state = vcat(state, refs)\n",
    "    end\n",
    "end\n",
    "\n",
    "function reward_function(env, name=nothing)\n",
    "    if name == \"my_ddpg\"\n",
    "        #println(\"Inside reward\")\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source2_v_C_filt\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source3_i_L1\", env.state_ids)]\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "        #state_to_control = [state_to_control_1, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "            refs = reference(env.t)\n",
    "            refs[1] = refs[1] / env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "            refs[2] = refs[2] / env.nc.parameters[\"source\"][2][\"v_limit\"]\n",
    "            refs[3] = refs[3] / env.nc.parameters[\"source\"][3][\"i_limit\"]\n",
    "\n",
    "            r = 1-1/3*(sum((abs.(refs - state_to_control)/2).^0.5))\n",
    "\n",
    "            #refs = reference(env.t)\n",
    "            #norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "            #r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "\n",
    "            #println(r)\n",
    "            return r\n",
    "        end\n",
    "    else\n",
    "        return 1\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "env = ElectricGridEnv(\n",
    "    CM = CM,\n",
    "    parameters = parameters,\n",
    "    t_end = 1,\n",
    "    featurize = featurize_ddpg,\n",
    "    reward_function = reward_function,\n",
    "    action_delay = 0,\n",
    "    verbosity = 2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a ddpg agent, just like shown in [the ReinforcementLearning.jl examples](https://juliareinforcementlearning.org/docs/experiments/experiments/Policy%20Gradient/JuliaRL_DDPG_Pendulum/#JuliaRL\\\\_DDPG\\\\_Pendulum). Note that we have to call `state` by `JEG.state` since `JEG` and `ReinforcementLearning` both export state and we imported them both at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = StableRNG(1)\n",
    "init = Flux.glorot_uniform(rng)\n",
    "\n",
    "ns = length(JEG.state(env, \"my_ddpg\"))#length(env.agent_dict[\"my_ddpg\"][\"state_ids\"])\n",
    "na = length(env.agent_dict[\"my_ddpg\"][\"action_ids\"])\n",
    "\n",
    "CreateActor() = Chain(\n",
    "    Dense(ns, 30, relu; init = init),\n",
    "    Dense(30, 30, relu; init = init),\n",
    "    Dense(30, na, tanh; init = init),\n",
    ") |> gpu\n",
    "\n",
    "CreateCritic() = Chain(\n",
    "    Dense(ns + na, 30, relu; init = init),\n",
    "    Dense(30, 30, relu; init = init),\n",
    "    Dense(30, 1; init = init),\n",
    ") |> gpu\n",
    "\n",
    "agent = Agent(\n",
    "    policy = DDPGPolicy(\n",
    "        behavior_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(),\n",
    "        ),\n",
    "        behavior_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(),\n",
    "        ),\n",
    "        target_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(),\n",
    "        ),\n",
    "        target_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(),\n",
    "        ),\n",
    "        γ = 0.99f0,\n",
    "        ρ = 0.995f0,\n",
    "        na = na,\n",
    "        batch_size = 64,\n",
    "        start_steps = 1000,\n",
    "        start_policy = RandomPolicy(-1.0..1.0; rng = rng),\n",
    "        update_after = 50,#1000,\n",
    "        update_freq = 1,\n",
    "        act_limit = 1.0,\n",
    "        act_noise = 0.001,\n",
    "        rng = rng,\n",
    "    ),\n",
    "    trajectory = CircularArraySARTTrajectory(\n",
    "        capacity = 10000,\n",
    "        state = Vector{Float32} => (ns,),\n",
    "        action = Float32 => (na, ),\n",
    "    ),\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our own agent, we can plug everything together and run the learning like we have already done it in the other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  10%|█████                                    |  ETA: 0:18:56\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "\r\u001b[32mProgress:  15%|███████                                  |  ETA: 0:12:11\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "\r\u001b[32mProgress:  25%|███████████                              |  ETA: 0:06:31\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The state(s) Any[\"source2_i_L1\"] exceeded limit(s)\n",
      "│                     -> episode abort\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:535\n",
      "┌ Warning: Corresponding limit(s): [629.5153538513777],\n",
      "│                     corresponding index: [3]\n",
      "└ @ JEG e:\\Documents\\dare\\src\\electric_grid_env.jl:537\n",
      "\r\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:11\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[97;1mTotal reward per episode\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n",
       "                 \u001b[38;5;8m┌────────────────────────────────────────┐\u001b[0m        \n",
       "         \u001b[38;5;8m328.685\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;2mmy_ddpg\u001b[0m\n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⡎\u001b[0m\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢰\u001b[0m\u001b[38;5;2m⠁\u001b[0m\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡜\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "   Score        \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⢀\u001b[0m\u001b[38;5;2m⡠\u001b[0m\u001b[38;5;2m⠇\u001b[0m⠀⠀\u001b[38;5;2m⢣\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀\u001b[38;5;2m⠁\u001b[0m⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m⠀\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⡄\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠈\u001b[0m\u001b[38;5;2m⠉\u001b[0m⠀\u001b[38;5;2m⢇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⠸\u001b[0m\u001b[38;5;2m⡀\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⡇\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "         \u001b[38;5;8m5.11507\u001b[0m \u001b[38;5;8m│\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;2m⢸\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;2m⣀\u001b[0m\u001b[38;5;8m│\u001b[0m \u001b[38;5;8m\u001b[0m       \n",
       "                 \u001b[38;5;8m└────────────────────────────────────────┘\u001b[0m        \n",
       "                 ⠀\u001b[38;5;8m0\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m\u001b[0m⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\u001b[38;5;8m20\u001b[0m⠀        \n",
       "                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both UnicodePlots and PlotlyJS export \"Plot\"; uses of it in module JEG must be qualified\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: Plot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Plot not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] RenderHookResults(; hook::DataHook, states_to_plot::Vector{Any}, actions_to_plot::Vector{Any}, plot_reward::Bool, plot_reference::Bool, episode::Int64, vdc_to_plot::Vector{Any}, power_p_inv::Vector{Any}, power_q_inv::Vector{Any}, power_p_poc::Vector{Any}, power_q_poc::Vector{Any}, v_mag_inv::Vector{Any}, v_mag_poc::Vector{Any}, i_mag_inv::Vector{Any}, i_mag_poc::Vector{Any}, freq::Vector{Any}, angles::Vector{Any}, i_sat::Vector{Any}, i_err::Vector{Any}, i_err_t::Vector{Any}, v_sat::Vector{Any}, v_err::Vector{Any}, v_err_t::Vector{Any}, v_dq::Vector{Any}, i_dq::Vector{Any}, return_plot::Bool)\n",
      "   @ JEG e:\\Documents\\dare\\src\\render.jl:346\n",
      " [2] top-level scope\n",
      "   @ e:\\Documents\\dare\\examples\\notebooks\\RL_Complex_DEMO_external_agent.ipynb:10"
     ]
    }
   ],
   "source": [
    "my_custom_agents = Dict(\"my_ddpg\" => agent)\n",
    "\n",
    "controllers = SetupAgents(env, my_custom_agents)\n",
    "\n",
    "hook_learn = DataHook(collect_state_ids = env.state_ids,\n",
    "                collect_action_ids = env.action_ids)\n",
    "\n",
    "Learn(controllers, env, num_episodes = 20, hook=hook_learn)\n",
    "\n",
    "RenderHookResults(hook = hook_learn,\n",
    "                    episode = 1,\n",
    "                    states_to_plot  = env.state_ids,\n",
    "                    actions_to_plot = env.action_ids,\n",
    "                    plot_reward=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we expect some differences since external agents do not support \"learning\" and \"non-learning\" mode. This means that even though we run a `Simulate` run we expect to see action noise in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = DataHook(collect_state_ids = env.state_ids,\n",
    "                collect_action_ids = env.action_ids)\n",
    "\n",
    "hook = Simulate(controllers, env, hook=hook)\n",
    "\n",
    "\n",
    "RenderHookResults(hook = hook,\n",
    "                    states_to_plot  = env.state_ids,\n",
    "                    actions_to_plot = env.action_ids,\n",
    "                    plot_reward=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
