{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2888dffe",
   "metadata": {},
   "source": [
    "## Train an RL agent controlling different sources\n",
    "In this notebook a reinforcement learning agent is trained control 3 sources. This notebook will focus the following topics:\n",
    "\n",
    "- ### Experiment one RL agent controlling different sources\n",
    "- ### Featurize and reward for all three sources\n",
    "- ### Train an agent to controll all three sources\n",
    " \n",
    "## Experiment one RL agent controlling different sources\n",
    "\n",
    "The experiment is shown in the figure below. Two sources will be feeded by a negative reference value and therefore act as active loads. The electrical power grid is here abstracted similar to the output of the `DrawGraph(env.nc)` methode.\n",
    "The sources and loads (here no loads are availabe) shown as colored circles connected via cables.\n",
    "This is similar like the usage of the GUI, where the sources, loads and cables can be parameterized and connected interactivly.\n",
    "\n",
    "![](figures/RL_Complex_Demo.png \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e01153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.webio.node+json": {
       "children": [],
       "instanceArgs": {
        "namespace": "html",
        "tag": "div"
       },
       "nodeType": "DOM",
       "props": {},
       "type": "node"
      },
      "text/html": [
       "<div style=\"padding: 1em; background-color: #f8d6da; border: 1px solid #f5c6cb; font-weight: bold;\">\n",
       "<p>The WebIO Jupyter extension was not detected. See the\n",
       "<a href=\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\" target=\"_blank\">\n",
       "    WebIO Jupyter integration documentation\n",
       "</a>\n",
       "for more information.\n",
       "</div>\n"
      ],
      "text/plain": [
       "WebIO._IJuliaInit()"
      ]
     },
     "metadata": {
      "application/vnd.webio.node+json": {
       "kernelId": "db246daa-cc5f-4685-b680-2841bc3e03e2"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f3322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 2 entries:\n",
       "  \"source\" => Any[Dict{Any, Any}(\"control_type\"=>\"RL\", \"mode\"=>\"my_ddpg\", \"fltrâ€¦\n",
       "  \"grid\"   => Dict{Any, Any}(\"phase\"=>1, \"ramp_end\"=>0.04)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = [0.0   1.0  0\n",
    "     -1.0   0.0  2.0\n",
    "     0  -2.0  0.0]\n",
    "\n",
    "parameters =\n",
    "Dict{Any, Any}(\n",
    "    \"source\" => Any[\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"control_type\" => \"RL\",\n",
    "                        \"mode\" => \"my_ddpg\",\n",
    "                        \"fltr\" => \"L\"),\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"fltr\" => \"LC\",\n",
    "                        \"control_type\" =>\n",
    "                        \"RL\", \"mode\" => \"my_ddpg\"),\n",
    "                    Dict{Any, Any}(\n",
    "                        \"pwr\" => 200e3,\n",
    "                        \"fltr\" => \"L\",\n",
    "                        \"control_type\" =>\n",
    "                        \"RL\", \"mode\" => \"my_ddpg\"),\n",
    "                    ],\n",
    "    \"grid\" => Dict{Any, Any}(\n",
    "        \"phase\" => 1,\n",
    "        \"ramp_end\" => 0.04,)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039119e",
   "metadata": {},
   "source": [
    "It can be noticed, that the control `mode` for all three sources is set to the same `my_ddpg` agent.\n",
    "\n",
    "\n",
    "As `reference(t)` function for simlicity, DC-values are used, one per source, since we are dealing with a single phase grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf3c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reference(t)\n",
    "    return [-10, 230, -15]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9ce89",
   "metadata": {},
   "source": [
    "## Featurize and reward for all three sources\n",
    "\n",
    "Afterwards the `featurize()` function adds the signal generated by the `reference` function to the state for the agent `my_ddpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23420f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1 (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_ddpg\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45180b",
   "metadata": {},
   "source": [
    "Then the `reward()` function is defined. Here, again it is based on the root-mean square error (RMSE) teach the agent `my_ddpg` to match the reference signal to the measured signal. \n",
    "\n",
    "If the measured state is greater than `1`. In that case a punishment is returned which, here, is chosen to be `r = -1`.\n",
    "It not and if the measured value differs from the reference, the average error is substracted from the maximal reward: `r = 1 - RMSE`:\n",
    "\n",
    "$r = 1 - \\frac{1}{3} \\sum_{{p \\in \\{\\mathrm{a,b,c}\\}}} \\sqrt{\\frac{|x_\\mathrm{ref,p} - x_\\mathrm{meas,p}|}{2}}$\n",
    "\n",
    "__Important here is the choise of the states!__\n",
    "\n",
    "For the first and third source the current thougth the inductors are used. \n",
    "Since the `reference` value is negative, the agent should learn to draw current from the grid.\n",
    "For the secound source, the voltag accros the capacitor is used and the reference value is positive. \n",
    "Therefore, the agent should learn to supply the capacitor / \"build up a grid\" (while the other 2 sources draw current!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5137a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward_function (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward_function(env, name = nothing)\n",
    "    if name == \"my_ddpg\"\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source2_v_C_filt\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source3_i_L1\", env.state_ids)]\n",
    "\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]    \n",
    "            # TODO: norm for v different!      \n",
    "            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "            return r \n",
    "        end\n",
    "    else\n",
    "        return 1\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb0752",
   "metadata": {},
   "source": [
    "## Train an agent to controll all three sources\n",
    "\n",
    "Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electircal power grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d1d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = ElectricGridEnv(\n",
    "    CM = CM,\n",
    "    parameters = parameters, \n",
    "    t_end = 0.1, \n",
    "    featurize = featurize_ddpg, \n",
    "    reward_function = reward_function, \n",
    "    action_delay = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ded2d",
   "metadata": {},
   "source": [
    "Like already shon in `RL_Single_Agent_DEMO.ipynb`, again an DDPG agent is created. \n",
    "The `SetupAgents()` function is then used to configure the `controllers` utilizing the `MultiController`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1089d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CreateAgentDdpg(na = length(env.agent_dict[\"my_ddpg\"][\"action_ids\"]),\n",
    "                          ns = length(state(env, \"my_ddpg\")),\n",
    "                          use_gpu = false)\n",
    "\n",
    "my_custom_agents = Dict(\"my_ddpg\" => agent)\n",
    "\n",
    "controllers = SetupAgents(env, my_custom_agents);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bf166",
   "metadata": {},
   "source": [
    "Here, the `controllers` struct constits only of the one `my_ddpg` agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7b14b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 1 entry:\n",
       "  \"my_ddpg\" => Dict{Any, Any}(\"policy\"=>typename(Agent)â€¦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controllers.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a41629",
   "metadata": {},
   "source": [
    "This agent puts out three different actions, one per source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76112dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Any}:\n",
       " \"source1_u\"\n",
       " \"source2_u\"\n",
       " \"source3_u\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controllers.agents[\"my_ddpg\"][\"action_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371d587",
   "metadata": {},
   "source": [
    "And has knowlegde about all the states of the three sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efb40db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Vector{Any}:\n",
       " \"source1_i_L1\"\n",
       " \"source1_v_C_cables\"\n",
       " \"source2_i_L1\"\n",
       " \"source2_v_C_filt\"\n",
       " \"source2_v_C_cables\"\n",
       " \"source3_i_L1\"\n",
       " \"source3_v_C_cables\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controllers.agents[\"my_ddpg\"][\"state_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab250b",
   "metadata": {},
   "source": [
    "The next steps are straigth forward compared to the prior RL example notebooks like `RL_Classical_Controllers_Merge_DEMO.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ff2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learn(controllers, env, num_episodes = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56598c5",
   "metadata": {},
   "source": [
    "After the training, the `Simulate()` function is used to run a test epiode without action noise and the state to be controlled ($i_\\mathrm{L1}$) is plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = DataHook(collect_state_ids = env.state_ids,\n",
    "                collect_action_ids = env.action_ids)\n",
    "\n",
    "hook = Simulate(controllers, env, hook=hook)\n",
    "\n",
    "\n",
    "RenderHookResults(hook = hook,\n",
    "                    states_to_plot  = env.state_ids,\n",
    "                    actions_to_plot = env.action_ids,\n",
    "                    plot_reward=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ab633d2ce8be4913891b08d5f284559f",
   "lastKernelId": "db246daa-cc5f-4685-b680-2841bc3e03e2"
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
